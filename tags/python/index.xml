<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on Grey Times</title>
    <link>http://kangkona.tk/tags/python/</link>
    <description>Recent content in Python on Grey Times</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Thu, 06 Feb 2014 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://kangkona.tk/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>NLPNote</title>
      <link>http://kangkona.tk/posts/nlp-note/</link>
      <pubDate>Thu, 06 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>http://kangkona.tk/posts/nlp-note/</guid>
      <description>

&lt;h2 id=&#34;1-python自然语言处理-笔记:f4f67061fc493a4c2b2b421639ba2c55&#34;&gt;1.《Python自然语言处理》笔记&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;遍历序列的各种方式&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    Python 表达式                       评论
    for item in s                       遍历 s 中的元素
    for item in sorted(s)               按顺序遍历 s 中的元素
    for item in set(s)                  遍历 s 中的无重复的元素
    for item in reversed(s)             按逆序遍历 s 中的元素
    for item in set(s).difference(t)    遍历在集合 s 中不在集合 t 的元素
    for item in random.shuffle(s)       按随机顺序遍历 s 中的元素

    组合使用显威力： reversed(sorted(set(s)))
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;zip &amp;amp;&amp;amp; enumerate&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    &amp;gt;&amp;gt;&amp;gt; words = [&#39;I&#39;, &#39;turned&#39;, &#39;off&#39;, &#39;the&#39;, &#39;spectroroute&#39;]
    &amp;gt;&amp;gt;&amp;gt; tags = [&#39;noun&#39;, &#39;verb&#39;, &#39;prep&#39;, &#39;det&#39;, &#39;noun&#39;]
    &amp;gt;&amp;gt;&amp;gt; zip(words, tags)
    [(&#39;I&#39;, &#39;noun&#39;), (&#39;turned&#39;, &#39;verb&#39;), (&#39;off&#39;, &#39;prep&#39;),
    (&#39;the&#39;, &#39;det&#39;), (&#39;spectroroute&#39;, &#39;noun&#39;)]
    &amp;gt;&amp;gt;&amp;gt; list(enumerate(words))
    [(0, &#39;I&#39;), (1, &#39;turned&#39;), (2, &#39;off&#39;), (3, &#39;the&#39;), (4, &#39;spectroroute&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对于一些 NLP 的任务,有必要将一个序列分割成两个或两个以上的部分。例如:我们
可能需要用 90%的数据来“训练”一个系统,剩余 10%进行测试。要做到这一点,我们指
定想要分割数据的位置,然后在这个位置分割序列。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    &amp;gt;&amp;gt;&amp;gt; text = nltk.corpus.nps_chat.words()
    &amp;gt;&amp;gt;&amp;gt; cut = int(0.9 * len(text)) 
    &amp;gt;&amp;gt;&amp;gt; training_data, test_data = text[:cut], text[cut:] 
    &amp;gt;&amp;gt;&amp;gt; text == training_data + test_data 
    True
    &amp;gt;&amp;gt;&amp;gt; len(training_data) / len(test_data) 4
    9
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我们可以验证在此过程中的原始数据没有丢失,也不是复制。我们也可以验证两块大
小的比例是我们预期的。&lt;/p&gt;

&lt;p&gt;让我们综合关于这三种类型的序列的知识,一起使用链表推导处理一个字符串中的词,
按它们的长度排序。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    &amp;gt;&amp;gt;&amp;gt; words = &#39;I turned off the spectroroute&#39;.split() 
    &amp;gt;&amp;gt;&amp;gt; wordlens = [(len(word), word) for word in words] 
    &amp;gt;&amp;gt;&amp;gt; wordlens.sort() 
    &amp;gt;&amp;gt;&amp;gt; &#39; &#39;.join(w for (_, w) in wordlens)
    &#39;I off the turned spectroroute&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;从html中提取信息的通用办法&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    import re
    def get_text(file):
    &amp;quot;&amp;quot;&amp;quot;Read text from a file, normalizing whitespace and stripping HTML markup.&amp;quot;&amp;quot;&amp;quot;
    text = open(file).read()
    text = re.sub(&#39;\s+&#39;, &#39; &#39;, text)
    text = re.sub(r&#39;&amp;lt;.*?&amp;gt;&#39;, &#39; &#39;, text)
    return text
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;防御性编程&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;     def tag(word):
        assert isinstance(word, basestring), &amp;quot;argument to tag() must be a string&amp;quot;
        if word in [&#39;a&#39;, &#39;the&#39;, &#39;all&#39;]:
            return &#39;det&#39;
        else:
            return &#39;noun&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;FreqDist&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;    def freq_words(file, min=1, num=10):
        text = open(file).read()
        tokens = nltk.word_tokenize(text)
        freqdist = nltk.FreqDist(t for t in tokens if len(t) &amp;gt;= min)
        return freqdist.keys()[:num]


    def freq_words_verbose(file, min=1, num=10, verbose=False):
     &#39;&#39;&#39;如果设置了 verbose 标志将会报告其进展情况&#39;&#39;&#39;
     freqdist = FreqDist()
     if trace: print &amp;quot;Opening&amp;quot;, file
     text = open(file).read()
     if trace: print &amp;quot;Read in %d characters&amp;quot; % len(file)
     for word in nltk.word_tokenize(text):
         if len(word) &amp;gt;= min:
            freqdist.inc(word)
            if trace and freqdist.N() % 100 == 0: print &amp;quot;.&amp;quot;
     if trace: print
     return freqdist.keys()[:num]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;调试＆＆Pdb&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;CSV :Python有自己的CSV库，csv.reader&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
&lt;p&gt;动态规划是一种在 NLP 中广泛使用的算法设计技术,它存储以前的计算结果,
以避免不必要的重复计算。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;2-文本分类:f4f67061fc493a4c2b2b421639ba2c55&#34;&gt;2. 文本分类&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;过拟合： 如果你提供太多的特征,那么该算法将高度依赖你的训练数据的特性，而一般化到新的例子的效果不会很好。这个问题被称为过拟合。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;devtest： 一旦初始特征集被选定,完善特征集的一个非常有成效的方法是错误分析。首先,我们选择一个开发集,包含用于创建模型的语料数据。然后将这种开发集分为训练集和开发测试集。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Topic映射到关键词组，映射的越多，表示越切题。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;不去关注词法，语法阶段的错误，做到隔离。面面俱到的效果未必好。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code&gt;    algorithm : 主题检测
    input: document
    tools: 
         - 特征提取器(eg:给定一些模式（词，词组，词模式)，得到对于给定document的`布尔数组`)
         - 训练一个文档分类器
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;特征提取函数的行为就像有色眼镜一样,强调我们的数据中的某些属性(颜色),并使其无法看到其他属性。分类器在决定如何标记输入时,将完全依赖它们强调的属性。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;要不要走一条无监督学习之路，寻找不动点。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;文本自动层级聚类
这种试探性的数据分析(exploratory data  analysis)来识别跑题作文,并辅以人工鉴别。这种内容评价方法的特点是不需要事先基于大规模标注训练集构建评价模型,并且有着层级聚合聚类法的突出优点,郎能够生成比较规整的类集合,聚类结果不依赖文档的初始排列或输入次序,与聚类过程的先后次序无关,聚类结果比较稳定,不易导致类的重构。并且对于作文i平价来讲,得到的结果比较容易解释。实验结果表明,该方法能比较清晰地识别与大多数作文内容不同的作文,再辅以人工鉴别,可准确识别跑题作文,从而在通用自动作文评价中实现作文内容的测量。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-代码工具库:f4f67061fc493a4c2b2b421639ba2c55&#34;&gt;3. 代码工具库&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;	# -*- coding: utf-8 -*-
	&amp;quot;&amp;quot;&amp;quot;
	为跑题检测储备的工具代码
	~/Project/NLTK-offTopic/.temp.py
	&amp;quot;&amp;quot;&amp;quot;
	import numpy as np
	import nltk as nk
	import re

	s=open(&amp;quot;nlptest.txt&amp;quot;,&amp;quot;r&amp;quot;)
	&#39;&#39;&#39;s.seek(0)&#39;&#39;&#39;
	m1=s.read()
	pattern=re.compile(&amp;quot;^A-Za-z&amp;quot;)
	s1=re.split(&amp;quot;\s+&amp;quot;,re.sub(pattern,&amp;quot; &amp;quot;,m1.replace(&amp;quot;\n&amp;quot;,&amp;quot; &amp;quot;)))
	s2=set(s1) #union作用
	s3=nk.FreqDist(s1)
	nk.FreqDist(s1).items()#显示所有信息
	nk.FreqDist(s1).keys()[:50]#显示前50的词频为降序排列
	s4=[len(ca) for ca in s1]#计算词频长度
	s5=nk.FreqDist(s4)
	max(s5)#显示最大的词频
	s5.freq(s5.keys(max(s5))#可显示比率
	z1=[ca for ca in set(s1) if s3[ca]&amp;gt;10]#可以只输出符合规则的词频
	s3.plot(50,cumulative=True)#累计汇总图前50个词频的
	s3.hapaxes()#输出只一次的词
	nk.bigrams(s1)#形成双连词
	s3.inc(s3)#可添加样本
	s3.N()可现实总量
	s3.tabulate(conditions=&amp;quot;表示第几个文本对应的名字&amp;quot;,samples=&amp;quot;表示要显示的词&amp;quot;)#绘制频率分布表
	 
	###nltk上的人机交互程序
	nk.chat.chatbots()
	 
	##载入自己的语料库
	 
	from nltk.corpus import PlaintextCorpusReader
	wd=&amp;quot;D:\\nlptest&amp;quot;
	wd1=PlaintextCorpusReader(wd,&amp;quot;.*&amp;quot;)
	wd1.fileids()#显示语料库中的文件名
	wd1.words(&amp;quot;nlptest.txt&amp;quot;)#显示词和split效果一样
	cfd=nk.ConditionalFreqDist((genre,word) for genre in [&amp;quot;first&amp;quot;,&amp;quot;second&amp;quot;] for word in s1)
	#对应的条件概率如果在word，其实就是分类汇总
	cfd.conditions()#显示并按照条件字母排序
	#一样可以用tabulate显示table列表
	 
	 
	#英文同义词字典库
	 
	from nltk.corpus import wordnet as wn
	wn.synsets(&amp;quot;hell&amp;quot;)#显示出同义词类
	wn.synset(&amp;quot;hell.n.01&amp;quot;).lemma_names#可以显示出在第几种词义词性下对应的相同英文
	wn.synset(&amp;quot;hell.n.01&amp;quot;).definition#词语定义
	wn.synset(&amp;quot;hell.n.01&amp;quot;).examples#常用词义例句子
	wn.synset(&amp;quot;hell.n.01&amp;quot;).lemmas#显示所有词条
	wn.lemma(&amp;quot;hell.n.01.hell_on_earth&amp;quot;)
	wn.lemma(&amp;quot;hell.n.01.hell_on_earth&amp;quot;).synset#显示上层hell.n.01
	wn.lemma(&amp;quot;hell.n.01.hell_on_earth&amp;quot;).name#显示单词&#39;hell_on_earth&#39;
	hell=wn.synset(&amp;quot;cat.n.01&amp;quot;)
	hell1=hell.hyponyms()
	sorted([lemma.name for synset1 in hell1 for lemma in synset1.lemmas])
	#Out[65]: [&#39;Felis_catus&#39;, &#39;Felis_domesticus&#39;, &#39;domestic_cat&#39;, &#39;house_cat&#39;, &#39;wildcat&#39;]
	#先进行词条分类
	#通过hyponyms找到下位词如毛 可以使什么类型的猫等
	hell.root_hypernyms()#可以找到最一般的上位词
	hell.hypernym_paths()#可以找到层次 len测是几类
	wn.synset(&amp;quot;cat.n.01&amp;quot;).part_meronyms()#可以显示出如猫耳朵，猫毛这类词汇关系
	wn.synset(&amp;quot;cat.n.01&amp;quot;).substance_meronyms()#不理解什么叫树的实质是心材和边材
	wn.synset(&amp;quot;walk.v.01&amp;quot;).entailments()#动词的蕴含意义，如走路蕴含着抬脚，吃蕴含着咀嚼等
	wn.lemma(&amp;quot;rush.v.01&amp;quot;).antonyms()#动词的反义
	dir(wn.synset(&amp;quot;rush.v.01&amp;quot;))#可以通过此种方式再词汇挂你选等上查找
	wn.synset(&amp;quot;right_whale.n.01&amp;quot;).lowest_common_hypernyms(wn.synset(&amp;quot;orca.n.01&amp;quot;))#可以计算出两个相似的类
	wn.synset(&amp;quot;right_whale.n.01&amp;quot;).min_depth()#可以查对应的深度量化
	wn.synset(&amp;quot;right_whale.n.01&amp;quot;).path_similarity(wn.synset(&amp;quot;orca.n.01&amp;quot;))#直接可以计算相似度1为基本一样
	&#39;&#39;&#39;urllib urlopen urlopen().read()
	#nk.word_tokenize()可直接对文本分词
	#text=nk.Text(nk.word_tokenize)的list形变为nltk文本
	list形式可用 .find(&amp;quot;&amp;quot;)
	 
	#html
	k1=urlopen().read()
	#直接转换
	k2=nk.clean_html(k1)
	k3=nk.word_tokenize(k2)
	&#39;&#39;&#39;
	 
	#词干提取器
	porter=nk.PorterStemmer()#或者LancasterStemmer()
	[porter.stem(t) for t in s1]
	#词型归并器
	l1=nk.WordNetLemmatizer()
	[l1.lemmatize(t) for t in s1]
	#nk.regexp_tokenize(text,pattern)比re.findall快很多
	#random.shuffle(sample)
	 
	#词性标注器
	nk.pos_tag(s1)#cc连词 rb副词 in介词 nn名词 jj形容词 vbp动词
	#adj形容词 adv动词 cnj连词 det限定词 ex存量词 fw外来词 mod情态动词
	#n 名词 np 专有名词 num数量词 pro代词 p介词 to 词投 uh感叹词 v动词
	#vd过去式 vg现有分词 vn过去分词 wh wh限定词
	 
	#读取已标注的语料库
	s1.tagged_words()
	#朴素贝叶斯
	cla=nk.NaiveBayesClassifier.train()
	cla.classfiy()
	nk.classify.accuracy(cla,&amp;quot;测试集&amp;quot;)
	#决策树
	nk.DecisionTreeClassifier.tarin()

&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>《Python学习笔记》的笔记</title>
      <link>http://kangkona.tk/posts/python-note-1/</link>
      <pubDate>Sat, 16 Mar 2013 00:00:00 +0000</pubDate>
      
      <guid>http://kangkona.tk/posts/python-note-1/</guid>
      <description>

&lt;p&gt;这是对雨痕写的Python学习笔记的学习。&lt;/p&gt;

&lt;h2 id=&#34;基本环境:92ca2ff6f04c0f4fcf9327caf7cf2980&#34;&gt;基本环境&lt;/h2&gt;

&lt;p&gt;对象总是按引用用传递,简单点说就是通过复制指针来实现多个名字指向同一一对象。
因为 arena 也是在堆上分配的,所以无无论何种类型何种大大小小的对象,都存储在堆上。Python 没有值类型和引用用类型一一说,就算是最简单的整数也是拥有标准头的完整对象。&lt;/p&gt;

&lt;h3 id=&#34;引用计数:92ca2ff6f04c0f4fcf9327caf7cf2980&#34;&gt;引用计数&lt;/h3&gt;

&lt;p&gt;除了直接引用用外,Python还支支持弱引用用。允许在不增加引用用计数,不妨碍对象回收的情况下间接引用用对象。但不是所有类型都支支持弱引用用,比比如 list、dict ,弱引用用会引发异常。&lt;/p&gt;

&lt;h3 id=&#34;垃圾回收:92ca2ff6f04c0f4fcf9327caf7cf2980&#34;&gt;垃圾回收&lt;/h3&gt;

&lt;p&gt;Python 拥有两套垃圾回收机制。除了引用计数,还有个专⻔门处理循环引用的 GC。通常我们提到垃圾回收时,都是指这个 &amp;ldquo;Reference Cycle Garbage Collection&amp;rdquo;。&lt;/p&gt;

&lt;p&gt;能引发循环引用用问题的,都是那种容器类对象,比比如 list、set、object 等。对于这类对象,虚拟机在为其分配内存时,会额外添加用用于追踪的PyGC_Head。这些对象被添加到特殊链表里里,以便GC 进行行管理。 如果不存在循环引用,自然是积极性更高的引用计数机制抢先给处理掉。也就是说,只要不存在循环引用,理论上可以禁用GC。当执行行某些密集运算时,临时关掉GC有助于提升性能。&lt;/p&gt;

&lt;p&gt;#世代回收&lt;/p&gt;

&lt;h2 id=&#34;内置类型:92ca2ff6f04c0f4fcf9327caf7cf2980&#34;&gt;内置类型&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;尽量使用xrange代替range&lt;/strong&gt;，每次迭代后,数字对象被回收,其占用用内存空闲出来并被复用用,内存也就不会暴涨了。&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;字符串:92ca2ff6f04c0f4fcf9327caf7cf2980&#34;&gt;字符串&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt;&amp;gt; &amp;quot;,&amp;quot;.join([&amp;quot;a&amp;quot;, &amp;quot;b&amp;quot;, &amp;quot;c&amp;quot;])
&#39;a,b,c&#39;
&amp;gt;&amp;gt;&amp;gt; &amp;quot;a,b,c&amp;quot;.split(&amp;quot;,&amp;quot;)
!
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]
&amp;gt;&amp;gt;&amp;gt; &amp;quot;a\nb\r\nc&amp;quot;.splitlines()
[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]
&amp;gt;&amp;gt;&amp;gt;&amp;quot;a\nb\r\nc&amp;quot;.splitlines(True)
[&#39;a\n&#39;, &#39;b\r\n&#39;, &#39;c&#39;]
&amp;gt;&amp;gt;&amp;gt; &amp;quot;abc&amp;quot;.startswith(&amp;quot;ab&amp;quot;), &amp;quot;abc&amp;quot;.endswith(&amp;quot;bc&amp;quot;)
True, True
&amp;gt;&amp;gt;&amp;gt; &amp;quot;abc&amp;quot;.upper(), &amp;quot;Abc&amp;quot;.lower()
&#39;ABC&#39;, &#39;abc&#39;

&amp;gt;&amp;gt;&amp;gt; &amp;quot;abcabc&amp;quot;.find(&amp;quot;bc&amp;quot;), &amp;quot;abcabc&amp;quot;.find(&amp;quot;bc&amp;quot;, 2) # 可指定查找起始结束位置。
1, 4
&amp;gt;&amp;gt;&amp;gt; &amp;quot; abc&amp;quot;.lstrip(), &amp;quot;abc &amp;quot;.rstrip(), &amp;quot; abc &amp;quot;.strip() # 剔除前后空格。
&#39;abc&#39;, &#39;abc&#39;, &#39;abc&#39;
&amp;gt;&amp;gt;&amp;gt; &amp;quot;abc&amp;quot;.strip(&amp;quot;ac&amp;quot;)  # 可删除指定的前后缀字符。
&#39;b&#39;
&amp;gt;&amp;gt;&amp;gt; &amp;quot;abcabc&amp;quot;.replace(&amp;quot;bc&amp;quot;, &amp;quot;BC&amp;quot;)# 可指定替换次数。
&#39;aBCaBC&#39;
&amp;gt;&amp;gt;&amp;gt; &amp;quot;a\tbc&amp;quot;.expandtabs(4)
&#39;a  bc&#39;
&amp;gt;&amp;gt;&amp;gt; &amp;quot;123&amp;quot;.ljust(5, &#39;0&#39;), &amp;quot;456&amp;quot;.rjust(5, &#39;0&#39;), &amp;quot;abc&amp;quot;.center(10, &#39;*&#39;) # 填充
&#39;12300&#39;, &#39;00456&#39;, &#39;***abc****&#39;
&amp;gt;&amp;gt;&amp;gt; &amp;quot;123&amp;quot;.zfill(6), &amp;quot;123456&amp;quot;.zfill(4) # 数字填充
&#39;000123&#39;, &#39;123456&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;string.Template可以处理较复杂的需求&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;池化:92ca2ff6f04c0f4fcf9327caf7cf2980&#34;&gt;池化&lt;/h3&gt;

&lt;p&gt;池化有助于减少对象数量和内存消耗,提升性能。用 intern() 函数可以把运行期动态生成的字符串池化。&lt;/p&gt;

&lt;h3 id=&#34;list:92ca2ff6f04c0f4fcf9327caf7cf2980&#34;&gt;List&lt;/h3&gt;

&lt;p&gt;可用bisect 向有序列表中插入元素。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;性能&lt;/strong&gt;
1. 列表用realloc()调整指针数组内存大小,可能需要复制数据。插入和删除操作,还会循环移动后续元素。这些都是潜在的性能隐患。对于频繁增删元素的大型列表,应该考虑用用链表等数据结构代替。
2. 某些时候,可以考虑用数组代替列表。和列表存储对象指针不同,数组直接内嵌数据,既省了创建对象的内存开销,又提升了读写效率。&lt;/p&gt;

&lt;p&gt;###tuple
在编码中,应该尽可能用用元组代替列表。除内存复用更高效外,其只读特征更利于并行开发。&lt;/p&gt;

&lt;p&gt;###dict
字典 (dict) 采用用开放地址法的哈希表实现。
• 自带元素容量为 8 的 smalltable,只有 &amp;ldquo;超出&amp;rdquo;时才到堆上额外分配元素表内存。
• 虚拟机缓存 80 个字典复用对象,但在堆上分配的元素表内存会被释放。
• 按需动态调整容量。扩容或收缩操作都将重新分配内存,重新哈希。
• 删除元素操作不会立立即收缩内存。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;对于大字典,调用 keys()、values()、items() 会构造同样巨大的列表。建议用用迭代器替代,以减
少内存开销。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&#34;视图:92ca2ff6f04c0f4fcf9327caf7cf2980&#34;&gt;视图&lt;/h4&gt;

&lt;p&gt;要判断两个字典间的差异,使用用视图是最简便的做法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; d1 = dict(a = 1, b = 2)
&amp;gt;&amp;gt;&amp;gt; d2 = dict(b = 2, c = 3)
&amp;gt;&amp;gt;&amp;gt; d1 &amp;amp; d2 # 字典不支支持该操作。
TypeError: unsupported operand type(s) for &amp;amp;: &#39;dict&#39; and &#39;dict&#39;
&amp;gt;&amp;gt;&amp;gt; v1 = d1.viewitems()
&amp;gt;&amp;gt;&amp;gt; v2 = d2.viewitems()
&amp;gt;&amp;gt;&amp;gt; v1 &amp;amp; v2  # 交集
set([(&#39;b&#39;, 2)])
&amp;gt;&amp;gt;&amp;gt; v1 | v2  #并
set([(&#39;a&#39;, 1), (&#39;b&#39;, 2), (&#39;c&#39;, 3)])
&amp;gt;&amp;gt;&amp;gt; v1 - v2  # 差集 (仅 v1 有,v2 没有的)
set([(&#39;a&#39;, 1)])
&amp;gt;&amp;gt;&amp;gt; v1 ^ v2  #对称差集 (不会同时出现在 v1 和 v2 中)
set([(&#39;a&#39;, 1), (&#39;c&#39;, 3)])
&amp;gt;&amp;gt;&amp;gt; (&#39;a&#39;, 1) in v1
True
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;set:92ca2ff6f04c0f4fcf9327caf7cf2980&#34;&gt;set&lt;/h3&gt;

&lt;p&gt;集合 (set) 用来存储无序不重复对象。所谓不重复对象,除了不是同一对象外,还包括 &amp;ldquo;值&amp;rdquo; 不能相同。集合只能存储可哈希对象,一样有只读版本 frozenset。&lt;/p&gt;

&lt;p&gt;运算： ==， ！=， &amp;gt;, |, &amp;amp; ,-, ^&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;数据结构很重要,这几个内置类型并不足以完成全部工作。像 C、数据结构、常用算法这类基础是每个程序开发人员都应该掌握的。&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
  </channel>
</rss>